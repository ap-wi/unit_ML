---
title: 'Coursera: Prediction Assignment Writeup'
author: "Cousera"
date: "Dezember 2017"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include=FALSE}

## install.packages( "devtools" )
library( devtools )
## devtools::install_github("tidyverse/dplyr")
library( tidyverse )

## install.packages( magrittr )
library( magrittr )

## install.packages( lubridate )
library( lubridate )

## install.packages( ggplot2 )
library( ggplot2 )

## install.packages( caret )
library( caret )

## install.packages( kernlab )
library( kernlab )

## install.packages( raster )
library( raster )

## install.packages( e1071 )
library( e1071 )

## install.packages( Metrics )
library( Metrics )

## install.packages( Hmisc )
library( Hmisc )

## install.packages( AppliedPredictiveModeling )
library( AppliedPredictiveModeling )

## install.packages( MASS )
library( MASS )

## install.packages( freqparcoord )
library( freqparcoord )

## install.packages( rattle )
library( rattle )
```

```{r include=FALSE}
## Functions:
## 
normal_data <- function( my_modFit ) {
## Normalization
   l_min <- summary(my_modFit)[1]; l_max <- summary(my_modFit)[6]
   pr_modFit <- 1 - ( ( l_max - my_modFit ) / ( l_max - l_min ) )
## Rückgabe   
   round( pr_modFit, digits = 4 )
}
```


## Executive Summary

The report explore die relationship between some variables of data from acelerometers on the belt, forearm, and dumbell of 6 participants. The goal is to predict the "classe" variable. For the prediction of the 20 predefined test data sets the method Support Vector Machine is used and delivers good results.


## Data Preparation und Data Expoloration

#### Data Source

More Information about the accelerometer data is available from the website:
http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har 

Training data: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

Test data: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The files should be made available in the current directory.

```{r echo=FALSE}
## read data
## training data
l_file_train <- "pml-training.csv"
training <- read.csv( l_file_train )
paste0( "size of ", l_file_train, " : ", nrow(training) )
## test data
l_file_test <- "pml-testing.csv"
testing <- read.csv( l_file_test )
paste0( "size of ", l_file_test, "  : ", nrow(testing) )
```


#### Data Preparation

First, the columns with missing data are eliminated.

```{r echo=FALSE}
## select columns
k <- 0; l_col_sel <- as.vector(NA)
for ( i in 1:ncol(testing) ) {
  if ( ( sum( is.na( training[,i] ) ) > 0.75 * nrow(training) ) || ( sum( is.na( testing[,i] ) ) > 0.75 * nrow(testing) ) ) { 
  } else {
    k <- k + 1
    l_col_sel[k] <- i
  }
}
```

```{r}
## select columns without missing values
my_train <- dplyr::select( training, l_col_sel ) 
my_test  <- dplyr::select( testing,  l_col_sel )

## names of selected columns
paste("Explanary Variables:")
paste("--------------------")
names(my_train)
```

The target variable "classe" is decomposed into 5 new numerical variables "classe_A", "classe_B", "classe_C", "classe_D" and "classe_E" for the prediction of the classification.

```{r}
## Target variables
my_train <- dplyr::mutate( my_train, classe_A = 0,
                                     classe_B = 0,
                                     classe_C = 0,
                                     classe_D = 0,
                                     classe_E = 0 )

my_train[ which( my_train$classe == "A") , ]$classe_A <- 1
my_train[ which( my_train$classe == "B") , ]$classe_B <- 1
my_train[ which( my_train$classe == "C") , ]$classe_C <- 1
my_train[ which( my_train$classe == "D") , ]$classe_D <- 1
my_train[ which( my_train$classe == "E") , ]$classe_E <- 1
```


#### Data Exploration

#### Parallel coordinates of explanary variables

By way of illustration, the explanatory variables are represented in a diagram of parallel coordinates.

```{r fig.height=8, fig.width=12}
freqparcoord::freqparcoord( x=my_train[ , c( 7:59 )] ,m=30, k=20, faceting="classe" )
```

#### Correlationen

Here is an overview of the correlations of the explanatory variables and the target variables:

```{r}
## correlations
cor_all <- abs( round( cor( x=my_train[,-c(1,2,5,6,60:65)], y=my_train[,c(61,62,63,64,65)] ), digits=4 ) )
```


#### Data for Training, Valuation and Test

```{r echo=FALSE}
inTrain <- createDataPartition( y=my_train$classe, p=0.75, list=FALSE )
my_training <- my_train[inTrain,]
my_validation <- my_train[-inTrain,]
paste0( "training data   : ", dim(my_training)[1], " observations / ", dim(my_training)[2], " variables" )
paste0( "validation data : ", dim(my_validation)[1], " observations / ", dim(my_validation)[2], " variables" )
paste0( "test data       : ", dim(my_test)[1], " observations / ", dim(my_test)[2], " variables" )
```


## Modeling


#### cross validation

The training data is divided into 3 subsets.

```{r echo=FALSE}
set.seed(32323)
## folder counts
c_k <- 3
## create folders
folds <- caret::createFolds( y=my_training$classe, k=c_k, list=FALSE, returnTrain=FALSE )
```

```{r echo=FALSE}
paste0("Size of ", c_k, "-folder subsets:")
table( as.factor( folds ) )
```

#### Classification of variable "classe_A"

The classification of the target variables "classe" or "classe_A", "classe_B", "classe_C", "classe_D" and "classe_E" is made by using the method Support Vector Machine. That Each expression of the characteristic "classe" is classified by a separate column. The new columns are created as numeric values. The larger the value, the more likely the classification to the "classe" and the smaller the less likely.

The function "tune.svm" allows the implicit use of a cross validation.

```{r}
l_exp <- my_train[ , c( 7:59, 61 )]
tune.resA <- tune.svm( classe_A ~ . , data = l_exp,
                       tunecontrol = tune.control( cross=c_k ) )
svmfitA <- tune.resA$best.model
```

```{r}
VmodfitA <- predict( svmfitA, newdata=my_validation, se.fit = TRUE, interval = "confidence" )
## Normalization
pr_VmodFitA <- normal_data( my_modFit=VmodfitA )
```

```{r}
## paste("Validation: residuals for classe_A:")
sum_svmfit <- summary(svmfitA$residuals)
```

```{r}
## predict test data
TmodFitA <- predict( svmfitA, newdata=my_test, se.fit = TRUE, interval = "confidence" )
## Normalization
pr_TmodFitA <- normal_data( my_modFit=TmodFitA )
pr_TmodFitA 
```


#### Classification of variable "classe_B"

Analogous to the classification of the variable "classe_A".

```{r echo=FALSE}
l_exp <- my_train[ , c( 7:59, 62 )]
tune.resB <- tune.svm( classe_B ~ . , data = l_exp, kernel="radial",
                       tunecontrol = tune.control(cross=c_k) )
svmfitB <- tune.resB$best.model
```

```{r include=FALSE}
VmodfitB <- predict( svmfitB, newdata=my_validation, se.fit = TRUE, interval = "confidence" )
## Normalization
pr_VmodFitB <- normal_data( my_modFit=VmodfitB )
```

```{r include=FALSE}
## paste("Validation: residuals for classe_B:")
sum_svmfit <- rbind( sum_svmfit, summary(svmfitB$residuals) )
```

```{r echo=FALSE}
## predict test data
TmodFitB <- predict( svmfitB, newdata=my_test, se.fit = TRUE, interval = "confidence" )
## Normalization
pr_TmodFitB <- normal_data( my_modFit=TmodFitB )
```


#### Classification of variable "classe_C"

Analogous to the classification of the variable "classe_A".

```{r echo=FALSE}
l_exp <- my_train[ , c( 7:59, 63 )]
tune.resC <- tune.svm( classe_C ~ . , data = l_exp, kernel="radial",
                       tunecontrol = tune.control(cross=c_k) )
svmfitC <- tune.resC$best.model
```

```{r include=FALSE}
VmodfitC <- predict( svmfitC, newdata=my_validation, se.fit = TRUE, interval = "confidence" )
## Normalization
pr_VmodFitC <- normal_data( my_modFit=VmodfitC )
```

```{r include=FALSE}
## paste("Validation: residuals for classe_C:")
sum_svmfit <- rbind( sum_svmfit, summary(svmfitC$residuals) )
```

```{r echo=FALSE}
## predict test data
TmodFitC <- predict( svmfitC, newdata=my_test, se.fit = TRUE, interval = "confidence" )
## Normalization
pr_TmodFitC <- normal_data( my_modFit=TmodFitC )
```


#### Classification of variable "classe_D"

Analogous to the classification of the variable "classe_A".

```{r echo=FALSE}
l_exp <- my_train[ , c( 7:59, 64 )]
tune.resD <- tune.svm( classe_D ~ . , data = l_exp, kernel="radial",
                       tunecontrol = tune.control(cross=c_k) )
svmfitD <- tune.resD$best.model
## table( l_exp$classe_D, predict(svmfitD) )
```

```{r include=FALSE}
VmodfitD <- predict( svmfitD, newdata=my_validation, se.fit = TRUE, interval = "confidence" )
## Normalization
pr_VmodFitD <- normal_data( my_modFit=VmodfitD )
```

```{r include=FALSE}
## paste("Validation: residuals for classe_D:")
sum_svmfit <- rbind( sum_svmfit, summary(svmfitD$residuals) )
```

```{r echo=FALSE}
## predict test data
TmodFitD <- predict( svmfitD, newdata=my_test, se.fit = TRUE, interval = "confidence" )
## Normalization
pr_TmodFitD <- normal_data( my_modFit=TmodFitD )
```


#### Classification of variable "classe_E"

Analogous to the classification of the variable "classe_A".

```{r echo=FALSE}
l_exp <- my_train[ , c( 7:59, 65 )]
tune.resE <- tune.svm( classe_E ~ . , data = l_exp, kernel="radial",
                       tunecontrol = tune.control(cross=c_k) )
svmfitE <- tune.resE$best.model
```

```{r include=FALSE}
VmodfitE <- predict( svmfitE, newdata=my_validation, se.fit = TRUE, interval = "confidence" )
## Normalization
pr_VmodFitE <- normal_data( my_modFit=VmodfitE )
```

```{r include=FALSE}
## paste("Validation: residuals for classe_E:")
sum_svmfit <- rbind( sum_svmfit, summary(svmfitE$residuals) )
```

```{r echo=FALSE}
## predict test data
TmodFitE <- predict( svmfitE, newdata=my_test, se.fit = TRUE, interval = "confidence" )
## Normalization
pr_TmodFitE <- normal_data( my_modFit=TmodFitE )
```


## Interpretation

#### Apply ml-algorithm to validation cases

Using the Confusion matrix, the quality of the classification is calculated on the validation data.


```{r}
table( as.factor( my_validation$classe ) )
```

```{r}
paste("Validation: residuals:")
dimnames(sum_svmfit)[[1]] <- c( "classe_A", "classe_B", "classe_C", "classe_D", "classe_E" )
print( sum_svmfit )
```

```{r echo=FALSE}
l_view_VFit <- cbind( pr_VmodFitA, pr_VmodFitB, pr_VmodFitC, pr_VmodFitD, pr_VmodFitE )
l_view_VFit <- round( l_view_VFit, digits=3 )
```

```{r echo=FALSE}
k <- nrow( l_view_VFit )
Vclasse <- vector()
Vprob   <- vector()
for ( i in 1:k ) {
  k <- which.max( l_view_VFit[i,] )
  if ( k == 1 ) { Vclasse[i] <- "A" }
  if ( k == 2 ) { Vclasse[i] <- "B" }
  if ( k == 3 ) { Vclasse[i] <- "C" }
  if ( k == 4 ) { Vclasse[i] <- "D" }
  if ( k == 5 ) { Vclasse[i] <- "E" }
  Vprob[i] <- round( l_view_VFit[i,k] / sum(as.numeric(l_view_VFit[i,])), digits=2 )
}  
```

```{r}
caret::confusionMatrix( data=my_validation$classe, Vclasse )
```

The key figures Accuracy and Kappa as well as Sensitivity and Specificity are very high for the validation data, so that the classification can run on the test data.


#### Apply ML-algorithm to 20 test cases

The classification of the expression of the "classe" takes place, in which the largest value within a row is determined from the 5 values (majority vote). Based on the magnitude, the reader can immediately see how stable or uncertain the classification is. The larger a value stands out, the more stable the classification. The more equally distributed the values within a row, the more uncertain the assignment.

```{r}
l_view_Fit <- cbind( pr_TmodFitA, pr_TmodFitB, pr_TmodFitC, pr_TmodFitD, pr_TmodFitE )
l_view_Fit <- round( l_view_Fit, digits=3 )
```

```{r echo=FALSE}
k <- nrow(l_view_Fit)
classe <- vector()
prob   <- vector()
for ( i in 1:k ) {
  k <- which.max( l_view_Fit[i,] )
  if ( k == 1 ) { classe[i] <- "A" }
  if ( k == 2 ) { classe[i] <- "B" }
  if ( k == 3 ) { classe[i] <- "C" }
  if ( k == 4 ) { classe[i] <- "D" }
  if ( k == 5 ) { classe[i] <- "E" }
  prob[i] <- round( l_view_Fit[i,k] / sum(as.numeric(l_view_Fit[i,])), digits=2 )
}  
l_view_Fit <- cbind( l_view_Fit, replicate( k, "->" ), classe, prob )
colnames(l_view_Fit) <- c("classe A", "classe B", "classe C", "classe D", "classe E", "   ", "classe", "with stability" )
```

```{r}
print( l_view_Fit, digits=3 )
```

Notice:
The assignment of the test case 3) is faulty, but the stability is with the value 0.37 rather small.
